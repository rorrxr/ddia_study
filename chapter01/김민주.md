# 데이터 중심 애플리케이션 설계 - 1장

## 들어가며: 왜 데이터 중심인가?

오늘날 많은 애플리케이션은 **CPU 연산 성능(compute-intensive)** 보다는 **데이터 중심적 문제(data-intensive)** 에 부딪힙니다.

> "얼마나 빨리 계산하느냐"보다 **데이터의 양, 복잡성, 변화 속도**가 더 큰 도전 과제

### 대부분의 애플리케이션이 필요로 하는 공통 기능

- 데이터를 저장하고 다시 찾아오기 (**Database**)
- 결과를 재사용하기 위해 캐싱 (**Cache**)
- 빠른 검색과 필터링을 위한 인덱스 (**Search Index**)
- 서비스 간 메시지 전달 (**Stream / Message Queue**)
- 대량 데이터를 분석하는 일괄 처리 (**Batch Processing**)

이처럼 많은 시스템 구성 요소가 필요하기 때문에, 이제 우리는 단순히 **개발자**가 아니라 **데이터 시스템 설계자**이기도 합니다.

---

## 데이터 시스템의 경계가 흐려진다

예전에는 **데이터베이스(DB)**, **캐시**, **메시지 큐** 등을 별개의 범주로 생각했지만, **최근에는 경계가 모호**해지고 있습니다.

| 도구 | 원래 용도 | 실제 사용 |
|------|----------|----------|
| **Redis** | 캐시 | 메시지 큐로도 사용 |
| **Kafka** | 메시지 큐 | 데이터베이스처럼 **영속성 보장** |
| **ElasticSearch** | 검색엔진 | 일부는 DB처럼 사용 |

### 핵심 변화

- 단일 도구로는 애플리케이션의 모든 요구사항을 충족시키기 어려움
- **여러 도구를 조합**해서 시스템을 만들어야 함
- 다양한 데이터 시스템들이 처리 방식에 따라 효율성을 위해 구간마다 나누어서 데이터를 관리
- 이러한 조합의 책임은 결국 **애플리케이션 코드**와 개발자에게 있음

---

## 데이터 시스템의 3대 목표

데이터 시스템을 설계할 때 가장 중요한 3가지 비기능적 요구사항:

### 🎯 **신뢰성(Reliability), 확장성(Scalability), 유지보수성(Maintainability)**

### 왜 이 세 가지인가?

**💡 "시간이 지나도 시스템이 살아남기 위한 최소 조건"**

| 원칙 | 핵심 질문 | 의미 |
|-----|---------|-----|
| **Reliability** | 오늘 제대로 동작하는가 | 현재의 안정성 |
| **Scalability** | 내일 더 커져도 동작하는가 | 미래의 성장 대응 |
| **Maintainability** | 모레 사람이 바뀌어도 고칠 수 있는가 | 지속적인 개선 가능성 |

---

## 1. 신뢰성 (Reliability)

### 1-1. 정의와 개념

**정의**: 시스템이 결함이 발생하더라도 **사용자가 기대한 기능을 올바르게 수행**하는 능력

#### 신뢰적인 시스템의 조건

- ✅ 애플리케이션은 사용자가 기대한 기능을 수행한다
- ✅ 시스템은 사용자의 실수나 예상치 못한 소프트웨어 사용법을 허용할 수 있다
- ✅ 시스템 성능은 예상된 부하와 데이터 양에서 필수적인 사용 사례를 충분히 만족한다
- ✅ 시스템은 허가되지 않은 접근과 오남용을 방지한다

> 무언가 잘못되더라도 지속적으로 올바르게 동작함을 신뢰성으로 이해합니다

---

### 결함(Fault) vs 장애(Failure)

| 용어 | 정의 | 범위 |
|-----|------|-----|
| **Fault(결함)** | 시스템의 한 구성 요소가 명세(사양)에서 벗어나 동작하는 상태 | 부분적 |
| **Failure(장애)** | 시스템 전체가 사용자에게 필요한 서비스를 제공하지 못하는 상태 | 전체적 |

#### 🎯 핵심 원칙

```
모든 fault를 완전히 제거하는 것은 불가능
↓
fault가 failure로 이어지지 않도록 설계하는 것이 중요
↓
이를 "내결함성(Fault-Tolerance)"이라고 부름
```

**결론**: 완전히 다 망가지는 것은 무조건 피해야 하고, **일부 시스템이 영향을 조금 받는 정도로 최소화**하는 것이 목표

---

### 의도적 Fault 발생의 중요성

실제로, 신뢰적인 시스템에서는 **의도적으로 fault를 발생**시키는 것이 도움이 될 수 있습니다:

#### 왜 일부러 장애를 만드는가?

- 많은 치명적 버그는 **오류 처리 미흡**에서 발생
- 일부러 장애를 유발하면 **내결함성 로직이 지속적으로 테스트**됨
- 실제 장애 상황에서도 올바르게 동작할 것이라는 **신뢰가 높아짐**

#### 💡 실제 사례: Netflix의 Chaos Monkey

```
무작위로 프로세스를 종료시켜 시스템의 회복 능력을 검증
→ 일부 서버를 랜덤으로 죽여서 내결함성을 훈련
```

---

### 예방이 더 중요한 경우

일반적으로는 **fault를 허용하고 복구하는 전략**이 선호되지만, 복구가 불가능한 경우에는 **예방이 최선**입니다.

#### 대표 사례: 보안 침해

- 민감한 데이터 유출은 **사후 복구가 불가능**
- 보안 버그는 **한 번 실수하면 끝**
- 이런 경우는 이 책의 스코프에서 제외
- 이 책은 주로 **복구 가능한 fault**를 다루며, 불완전한 구성 요소들로도 신뢰적인 시스템을 구축하는 방법을 설명

---

## 1-2. 하드웨어 결함 (Hardware Faults)

### 현실적 통계

| 지표 | 값 | 의미 |
|-----|-----|-----|
| **하드디스크 MTTF** | 10~50년 | 평균 고장 간격 |
| **10,000개 클러스터** | 하루 평균 1개 고장 | 대규모에서는 상시 발생 |

> 대규모 데이터센터에서는 하드웨어 문제가 **상시적으로 발생**

### 과거 vs 현재의 접근 방식

#### ⏮ 과거: 단순 중복(Redundancy)

서버가 사내에 보통 한 대여서 기초적인 복제 정도로 충분:
- RAID 구성
- 핫스왑 CPU
- 듀얼 전원 공급 장치
- 배터리/디젤 발전기 백업 전기

#### ⏩ 현재: 내결함성 전제 설계

- 데이터 규모가 많이 커짐
- **내결함성(fault-tolerance)을 전제**로 설계해야 함
- **중복(Replication, RAID)** 으로 해결

---

## 1-3. 소프트웨어 오류 (Software Errors)

### 하드웨어 vs 소프트웨어 결함의 차이

| 구분 | 하드웨어 결함 | 소프트웨어 결함 |
|-----|------------|-------------|
| **범위** | 부분적 (인스턴스 하나) | **전체 시스템** |
| **파급력** | 낮음 (로드밸런서가 커버) | **매우 높음** |
| **이유** | 독립적 고장 | 버그는 모든 배포 버전에 동일하게 영향 |

> 소프트웨어 결함은 **훨씬 큰 장애로 이어질 가능성**이 높음

### 🔍 흔한 패턴: 엣지 케이스 잠복

- 특정 입력에서 전체 서비스가 다운되는 버그
- 평소에는 발견되지 않다가 **특정 조건에서만 나타남**

---

### 해결 방법

> ⚠️ **완전한 해결책은 없음** - 여러 작은 노력들의 조합이 필요

#### 주요 대응 전략

1. **신중한 설계**
   - 시스템 가정과 컴포넌트 간 상호작용을 신중히 설계

2. **철저한 테스트**
   - 특히 **경계 조건(edge case)** 테스트

3. **프로세스 격리**
   - 한 프로세스의 문제가 전체로 번지지 않도록

4. **자동 재시작**
   - 장애 발생 시 크래시 후 재시작 허용

5. **관측 가능성**
   - 운영 환경에서의 측정, 모니터링, 로그 분석

---

## 1-4. 인적 오류 (Human Errors)

### 현실: 사람이 가장 큰 문제

| 통계 | 의미 |
|-----|-----|
| **1위 오류** | 사람의 설정(config) 오류 |
| **하드웨어 오류** | 10~25%에 불과 |

> 운영자, 개발자의 실수(human error)가 **가장 큰 원인**

---

### 해결 방법

#### 1️⃣ 실수를 유발하기 어려운 설계

```
잘 설계된 추상화, API, 관리자 인터페이스
↓
"올바른 사용"은 쉽게, "잘못된 사용"은 어렵게
↓
⚠️ 단, 너무 제한적이면 우회 사용이 생기므로 균형이 중요
```

💭 **생각**: 디자인 시스템 등과 같은 것도 포함. 거의 모든 설계를 아우르는 느낌의 것이라 꽤 넓은 범위

---

#### 2️⃣ 실험과 운영 환경의 분리

- 실수가 잦은 작업을 실제 장애로 직결되지 않게 분리
- 실제 데이터로 실험 가능한 **비운영(Sandbox) 환경** 제공
- 운영 환경에 영향을 주지 않고 안전한 실험 가능
- 롤백, 샌드박스 환경, 단계적 배포(예: **카나리 배포**)

💭 **생각**: 매우 좋은 방법. dev/staging에서 잡히는 경우가 매우 많음

---

#### 3️⃣ 철저한 테스트

```
단위 테스트 → 통합 테스트 → 전체 시스템 테스트 → 수동 테스트
```

- 자동화 테스트는 잘 알려져 있음
- 특히 **드물게 발생하는 경계 상황**을 검증하는 데 효과적

💭 **생각**: 
- config는 테스팅이 어렵지 않은가? 
- **production에서만 적용되는 config들이 진짜 제일 취약함**
- 이걸 테스트로 관리가 가능한가? 
- 테스트라기보다는 **staging 서버가 실제 production과 얼마나 비슷하냐**에 달린 듯
- 비즈니스 로직 버그는 무조건 없음을 보장해야 함
- 그렇기에 테스팅이 언급되었다고 생각

---

#### 4️⃣ 빠른 복구를 가능하게 하는 설계

- ⏪ 설정 변경 신속 롤백
- 📈 새 코드의 점진적 배포 (문제 발생 시 영향 최소화)
- 🔄 잘못된 계산을 바로잡을 수 있는 데이터 재계산 도구 제공

---

#### 5️⃣ 모니터링과 관측 가능성

- 성능 지표, 에러율 등 정교하고 명확한 모니터링
- 다른 공학 분야에서는 이를 **텔레메트리(telemetry)** 라 부름
- 이상 징후 조기 감지
- 가정과 제약 조건 위반 여부 확인
- 장애 발생 시 원인 분석에 핵심적

---

#### 6️⃣ 관리와 교육

- 좋은 관리 체계와 지속적인 교육 역시 중요
- 다만 이는 기술적 주제 범위를 넘어 이 책에서는 깊이 다루지 않음

---

## 1-5. 신뢰성은 얼마나 중요한가?

### 일상적인 애플리케이션에서도 신뢰성은 매우 중요

| 영역 | 영향 |
|-----|-----|
| 항공, 미사일 | 당연히 중요 |
| 드라이브 앱 | 아기 사진 유일 보관 → 데이터 손실 시 치명적 |
| 비즈니스 앱 | 생산성 손실, 잘못된 수치 보고 시 법적 리스크 |
| 전자상거래 | 직접적인 매출 손실과 브랜드 신뢰도 하락 |

> ⚠️ 예민한 분야에만 중요한 것이 **아님**

---

## 2. 확장성 (Scalability)

### 2-1. 부하 기술하기 (Describing Load)

**정의**: 부하(Load)가 증가해도 시스템 성능을 유지할 수 있는 능력

### 🎯 핵심: 올바른 접근 방식

#### ❌ 잘못된 질문
> "X 시스템은 확장 가능하다"

#### ✅ 올바른 질문
> "부하가 특정 방식으로 증가할 때 어떤 선택지가 있으며, 자원을 어떻게 추가해야 성능을 유지할 수 있을까?"

---

### 부하 매개변수(Load Parameter)

확장성을 논하기 전에 먼저 **현재 시스템 부하를 정량적으로 표현**할 수 있어야 합니다.

| 시스템 유형 | 부하 파라미터 |
|-----------|-------------|
| 웹 서버 | 초당 요청 수 (QPS, RPS) |
| DB | 읽기:쓰기 비율 |
| 채팅 서비스 | 동시 활성 사용자 수 |
| 캐시 | 적중률(hit ratio) |

### ⚠️ 이 파트의 핵심

> **"load parameters"(부하 파라미터)를 잘못 잡으면 망함**

---

## 📌 트위터 사례: 팬아웃(Fan-out) 문제

### 표면적 지표

| 작업 | 평균 | 피크 | 실제 |
|-----|-----|-----|-----|
| **트윗 작성 (쓰기)** | 4,600 req/s | 12,000 req/s | 어렵지 않음 |
| **홈 타임라인 (읽기)** | 300,000 req/s | - | **병목 구간** |

### 진짜 문제: 팬아웃(Fan-out)

> 쓰기 자체는 어렵지 않지만, **문제의 핵심은 팔로워 수에 따른 확산(fan-out)**

---

### 접근 방식 1: 읽기 시점 계산 (Read-time fan-out)

```
사용자가 타임라인 요청
↓
팔로우한 사용자들의 트윗을 모두 가져옴
↓
합치고 정렬
↓
⚠️ 읽기 요청이 폭발적일 때 병목 발생
```

---

### 접근 방식 2: 쓰기 시점 팬아웃 (Write-time fan-out)

```
사용자가 트윗 작성
↓
모든 팔로워의 타임라인 캐시에 미리 삽입
↓
✅ 읽기는 빠름
⚠️ 팔로워가 수천만 명인 경우 쓰기 비용 폭발
```

---

### ✅ 실제 해결책: 혼합 전략

트위터는 처음에는 **방식 1** 이었다가, 다음과 같이 변경:

| 사용자 유형 | 전략 | 이유 |
|-----------|-----|-----|
| **일반 사용자** | 쓰기 시점 팬아웃 (방식 2) | 팔로워 수 적음 |
| **유명인** | 읽기 시점 계산 (방식 1) | 수천만 팔로워 → 쓰기 비용 폭발 방지 |

### 🎯 핵심 부하 파라미터

> **사용자별 팔로워 수 분포**

핵심 부하 파라미터를 잘못 잡는다면 **설계가 잘못될 가능성 매우 높아짐**

---

### 💡 인사이트

확장성은 정답이 아니라 **워크로드 특성에 맞는 트레이드오프 선택**

💭 **추가 핵심**: 
- System design은 **ROI를 따지는 리스크 매니지먼트**에 가까움
- 성능은 적당히 좋아도 됨
- **수많은 변수를 생각**해야 함

---

## 2-2. 성능 기술하기 (Describing Performance)

### ⚠️ 이 파트의 핵심

> 부하 파라미터를 **잘못된 방식으로 해석**하면 또 망한다

### 시스템 유형별 성능 지표

| 시스템 유형 | 핵심 지표 | 설명 |
|-----------|---------|-----|
| **배치 처리** (예: Hadoop) | **처리량(Throughput)** | 초당 처리 레코드 수 또는 전체 작업 완료 시간 |
| **온라인 시스템** | **응답 시간(Response Time)** | 요청을 보낸 시점부터 응답을 받기까지의 시간 |

---

### 응답 시간은 분포다

#### 🎯 핵심 개념

> **응답 시간은 단일 값이 아니라 분포(distribution)**

#### 조심해야 할 두 가지

1. **응답 시간을 대표하는 단일 지표를 찾아서 보려는 노력**
   - 분포를 파악하는 데 방해가 될 수 있음
   - **분포 파악을 위해 여러 지표를 봐야 함**

2. **분포 파악을 위해 보는 지표들도 잘 선별해야 함**

---

## 평균 vs 백분위(Percentile)

### ❌ 평균 응답 시간(mean)은 쓰레기 값

#### 왜 평균이 의미 없는가?

```
응답 시간 분포는 uniform distribution이 아님
↓
느린 애들이 극소수로 몰려있는 지수/로그 함수 형태
↓
소득 평균 계산처럼 의미 없는 값
```

**예시**:
- 평균 응답 시간 = 200ms
- 하지만 상위 1% 요청은 3초 이상 지연될 수 있음

> ⚠️ **평균만 보는 것은 위험**

---

### ✅ 백분위(Percentile)를 봐야 함

| 지표 | 의미 | 중요도 |
|-----|------|-------|
| **p50 (중앙값, median)** | 절반의 요청이 이 시간보다 빠름 | 기본 |
| **p95** | 상위 5%를 제외한 95%의 요청이 이 시간 안에 처리됨 | 중요 |
| **p99** | 상위 1%를 제외한 99%의 요청이 이 시간 안에 처리됨 | **매우 중요** |
| **p99.9** | 상위 0.1%를 제외한 요청의 응답 시간 | 대규모 시스템 |

> 여러 개를 고루 봐야 함. 꼭 하나만 빠르게 본다면 **p99 기준**이 다른 것보다 나음

---

## 왜 p99가 중요한가?

### 💡 아마존 사례

#### 핵심 인사이트

> **가장 불편한 사용자가 보통 가장 데이터가 많은 헤비 유저**
> 
> **돈을 가장 많이 벌어주는 고객**

#### 구체적 수치

| 지표 | 영향 |
|-----|-----|
| 응답 시간 **100ms 증가** | 매출 **1% 감소** |
| 응답 시간 **1초 증가** | 고객 만족도 **16% 감소** |

#### 아마존의 SLA

- 내부 SLA: **"p99 응답 시간 < 1초"**

> 그렇기에 **p99 기준으로 엄격하게** 바라봐야 함

---

### p99.9는 어떨까?

| 백분위 | 사용 여부 | 이유 |
|-------|---------|-----|
| **p99.9** | 회사 스케일에 따라 필요 | 대규모 시스템 |
| **p99.99** | 엔터프라이즈 급에서도 잘 안 봄 | 만 번에 한 번 발생하는 특수 케이스<br/>반복 패턴이 없어서 고도화 비용이 너무 높음 |

---

### 꼬리 지연(tail latency)의 중요성

> **소수 사용자만 겪더라도 실제 비즈니스에 큰 영향**을 줌

---

## 추가 주의점

### 1️⃣ 백엔드 서버 여러 개일 때

```
❌ Percentile끼리 평균내면 안 됨
↓
✅ 분포 자체를 합치는 게 맞음 (== 히스토그램 합치기)
```

### 2️⃣ 부하 테스트 방법

```
❌ 응답 기다리고 다음 요청 보내기
↓
✅ 응답 기다리지 말고 계속 보내기
   (k6 같은 도구 사용 시 자동 처리)
```

### 3️⃣ SLA, SLO 활용

- **SLA**에서 지표로 사용 가능
- SLA에서 못 지키면 **고객들에게 환불/보상**할 기준이 됨

---

## 2-3. 부하 대응 방식 (Approaches for Coping with Load)

### 🎯 핵심 원칙

> **한 부하 수준에 적합한 아키텍처는 10배 부하에서는 거의 확실히 통하지 않는다**

#### 성장하는 서비스의 현실

- 빠르게 성장하는 서비스라면 **부하가 한 자릿수 배 증가할 때마다 아키텍처 재검토가 필요**

💭 **생각**: 
- 최근 GitHub 리드 개발자 팟캐스트에서 본 내용
- **0 하나 붙인 것까지는 생각함**
- 근데 그 이상은? 당장은 생각 안 할 가능성 높음

---

## Scale Up vs Scale Out

### 두 가지 접근 방식

| 방식 | 이름 | 설명 | 장단점 |
|-----|-----|-----|-------|
| **수직 확장** | **Vertical Scaling** | 더 강력한 장비로 교체<br/>(CPU 코어, 메모리 증설) | ✅ 단순<br/>❌ 비용 급격 증가 |
| **수평 확장** | **Horizontal Scaling** | 더 많은 장비로 부하를 분산<br/>(비공유 아키텍처) | ✅ 대규모 시스템 필수<br/>❌ 복잡도 증가 |

---

### ✅ 실무 접근: 균형 잡기

```
❌ 한쪽만 사용
↓
EC2 200개보다 적당히 큰 인스턴스 몇 개만 쓰는 게 더 안정적
↓
즉, 둘의 밸런스를 맞추는 편
```

---

## 자동 확장(Elastic)

### 탄력적 확장의 실제

- **탄력적(Elastic) 확장**: 자동으로 자원 추가/제거

#### 자동 확장이 꼭 좋은 것은 아님

```
자동 확장의 문제
↓
예측 못할 Load Balancer 오토 파일럿 이슈
↓
수동 확장이 오히려 더 안정적일 수 있음
```

> 운영 단순성을 위해 **수동 확장을 선택**하는 경우도 많음

---

## 핵심 인사이트

### 1️⃣ 범용 해답은 없다

> 확장성에는 **범용 해답이 없다**

### 2️⃣ 워크로드 특성 파악

시스템의 **주요 부하 특성**에 맞는 전략을 선택:
- 읽기/쓰기 비율
- 데이터 크기
- 응답 요구사항

### 3️⃣ 스타트업의 현실

```
초기 단계라면
↓
미래의 부하를 과도하게 대비하기보다
↓
빠른 기능 개선과 반복이 더 중요
```

---

## 3. 유지보수성 (Maintainability)

### 정의

**정의**: 시간이 지나도 시스템이 쉽게 운영되고, 수정·개선될 수 있는 능력

---

## 왜 중요한가?

### 비용의 진실

> **소프트웨어 비용의 대부분은 초기 개발이 아니라 유지보수에서 발생**

### 유지보수에 포함되는 것들

- 버그 수정
- 시스템 운영 및 장애 대응
- 장애 원인 분석
- 새로운 플랫폼 대응
- 새로운 요구사항/유즈케이스 추가
- 기술 부채 상환
- 기능 확장

---

## 레거시 시스템의 문제

### 왜 개발자들이 레거시를 싫어하는가?

| 이유 | 설명 |
|-----|-----|
| 다른 사람이 만든 복잡한 코드 | 이해하기 어려움 |
| 더 이상 최신이 아닌 기술 스택 | 구식 기술 |
| 원래 의도와 다르게 확장된 시스템 | 설계 의도 불명확 |

### 핵심 인사이트

```
레거시 시스템은 각자 다르게 불쾌
↓
만능 해결책은 없다
↓
하지만 애초에 레거시를 만들지 않도록 설계하는 것은 가능
```

---

## 소프트웨어 시스템 설계 세 가지 원칙

### 1️⃣ 운용성 (Operability): 운영팀의 삶을 편하게

#### 이 파트의 핵심

> **운영 용이성은 깔끔한 옵저버빌리티, 이해하기 쉬운 인프라가 80% 이상을 차지함**

#### 운용성이란?

- 운영자가 시스템을 안정적으로 관리할 수 있어야 함
- 좋은 모니터링, 자동화, 장애 대응 절차 필요

---

#### 좋은 운영성을 위해 필요한 것

| 요소 | 설명 |
|-----|-----|
| **가시성** | 시스템 동작에 대한 모니터링 |
| **자동화** | 자동화와 표준 도구 지원 |
| **문서화** | 예측 가능한 동작과 좋은 문서화 |
| **설정** | 좋은 기본 설정값 (필요시 재정의 가능) |
| **자가 치유** | 자가 치유 능력 (수동 개입 최소화) |

---

### 2️⃣ 단순성 (Simplicity): 복잡도 관리

#### 이 파트의 핵심

- **시스템 레벨에서의 개념 수 최소화**를 해야 한다 (good abstractions)
- **깔끔한 코드와 파일 구조**가 유지보수성을 좌우한다
- **필요 없는 복잡도를 제거**하는 모든 행위가 Simplicity

---

#### 단순성이란?

- 불필요한 복잡도를 줄여야 함
- 좋은 추상화(SQL, API, 언어 레벨 추상화)는 복잡도를 감춤
- 복잡도가 쌓이면 **"Big Ball of Mud(진흙 덩어리)"** 가 됨

---

#### 복잡도의 증상

| 증상 | 설명 |
|-----|-----|
| **상태 공간 폭발** | 가능한 상태가 너무 많음 |
| **강한 결합** | 모듈 간 강한 결합 |
| **복잡한 의존성** | 의존성 그래프가 복잡함 |
| **일관성 없는 명명** | 일관성 없는 명명과 용어 |
| **해킹** | 성능 문제 해결을 위한 해킹 |
| **임시방편** | 임시방편으로 추가된 특수 케이스 |

---

#### 좋은 추상화란?

```
구현 세부사항을 깔끔한 인터페이스 뒤에 숨김
```

| 예시 | 숨기는 것 |
|-----|---------|
| **SQL** | 복잡한 디스크/메모리 구조 → 단순한 쿼리 언어 |
| **고수준 언어** | 기계어, 레지스터, 시스템 콜 |

---

💭 **생각**:
- 사실상 모든 코드 상의 가독성/디자인 패턴이 다 이 항목에 들어가는 듯함
- good abstractions → 디자인 패턴이라고 생각해봤는데, 알고 보니 **시스템 레벨에서의 개념 수가 핵심**인 듯함

---

### 3️⃣ 발전성 (Evolvability): 변화를 쉽게

#### 이 파트의 핵심

- **큰 시스템도 코드/기능 수정처럼 변화될 수 있어야 함**
- **Evolvability란 "변경이 일어날 것을 전제로 한 설계 능력"**

---

#### 발전성이란?

- 요구사항 변화에 쉽게 적응할 수 있어야 함
- 리팩토링, 애자일, 테스트 주도 개발(TDD) → 시스템을 변화에 강하게 만듦

---

#### 비즈니스 요구사항은 지속적으로 변화

| 변화 요인 | 예시 |
|---------|-----|
| 새로운 사실 발견 | 기존 가정이 틀렸음 |
| 예기치 못한 유즈케이스 | 사용자의 새로운 요구 |
| 비즈니스 우선순위 변경 | 전략 변경 |
| 사용자 요청 | 기능 추가 요청 |
| 새로운 플랫폼 | 모바일, 웨어러블 등 |
| 법적/규제 요구사항 | GDPR, 개인정보보호법 등 |
| 성장에 따른 아키텍처 변경 | 스케일 증가 |

---

#### Agile 관점

```
기능/코드 수정
↓
TDD/애자일 등의 이론으로 많이 다루어짐 (매우 흔한 토픽)
↓
하지만
↓
대규모 시스템의 아키텍처도 똑같이 쉽게 마이그레이션 가능해야 함
(트위터의 방법론 1, 2 예시처럼)
```

---

## 쉬운 비유: 데이터 시스템 = 도시 인프라

데이터 시스템을 도시 인프라에 비유하면 이해가 쉽습니다.

| 데이터 시스템 | 도시 인프라 | 역할 |
|------------|----------|-----|
| **데이터베이스(DB)** | 수도관 | 항상 물을 저장하고 공급 |
| **캐시(Cache)** | 물탱크 | 빠른 공급을 위해 임시 저장 |
| **메시지 큐** | 택배 물류 | 비동기 전달 |
| **검색엔진** | 내비게이션 | 빠른 길 찾기 |
| **배치 처리** | 야간 쓰레기 수거 | 대량 작업을 한 번에 처리 |

### 핵심 유사성

```
도시가 커질수록
↓
수도, 전기, 교통을 확장해야 하는 것처럼
↓
애플리케이션도
↓
확장성, 신뢰성, 유지보수성을 확보해야 함
```

---

## 📝 정리

### 1️⃣ 신뢰성 (Reliability)

> **결함이 있어도 시스템은 멈추지 않는다**

- **Fault ≠ Failure**
- **하드웨어, 소프트웨어, 인적 오류** 모두 대응 필요
- **의도적 fault 발생**으로 내결함성 테스트 (Chaos Monkey)

---

### 2️⃣ 확장성 (Scalability)

> **부하 증가에도 성능을 유지할 수 있는 전략 필요**

- **부하 파라미터를 정확히 파악**하는 것이 핵심
- **평균이 아닌 백분위**(p50, p95, p99)로 성능 측정
- **Scale Up과 Scale Out의 균형**

---

### 3️⃣ 유지보수성 (Maintainability)

> **장기적으로 엔지니어가 쉽게 다룰 수 있어야 한다**

- **운용성**: 깔끔한 모니터링과 자동화
- **단순성**: 불필요한 복잡도 제거, 좋은 추상화
- **발전성**: 변화에 대응할 수 있는 설계

---
